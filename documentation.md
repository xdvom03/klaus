# Temporary

This will eventually become a proper PDF and less of a rambling mess. The same *caveat emptor* applies here as to the program, but even more. This is possibly outdated, incorrect, unrealised, unorganised or just un-.

# Timeline

This project will be in development until at least February, most likely March. Deadline for school is March 26th, with further fate unclear as of now. Ongoing modifications usually reflect the posted issues.

# Purpose of this program

Klaus is a Bayesian classifier meant to be used for classifying the World Wide Web. It should be able to detect the class of a given site, typically reflecting its topic, style, or purpose. This information is then used to search for sites belonging to a given class.

# Bayesian classification

## Tokenization

A site is classified based on its textual contents. HTML is downloaded, stripped of all scripts, styles, and tags (in that order), and the resulting text is cleaned of most interpunction and downcased. This produces a list of words for the site, which are then used to determine its placement. To avoid a single page overly spamming a word beyond usability (for we only care about contains/does-not-contain), words are only placed into the corpus once for a given document. To avoid thus underestimating the count of very common words on long websites, the text is first split into chunks of a given length, and only then does the unduplication happen. 

## Corpora

Each class contains example sites. A corpus is generated by tokenizing all sites and adding up word counts. Each class thus has an associated corpus with the number of occurrences of each word in its documents.

## Two classes

We can decide between two classes using a Naive Bayes algorithm. For naming purposes, one of the folders is the "target", while the other is the "control". Their corpora are scaled to contain the same number of words overall (the larger folder has all its occurrence counts divided by the word count ratio).

When trying to decide whether a given text belongs to the target class, we also split it into tokens and look for the "score" of each token. The score represents the probability that a document with the word occurs in the target class. If the word occurs T times in the target class and C times in the control class, the score is (T + k)/(T + C + 2k), where k is a smoothing parameter ("Lidstone smoothing") meant to prevent probabilities of zero (k = 1 is used ("Laplace smoothing")).

Combining evidence from all words can lead to amplifying noise. Instead, only the X worst scores for each folder are considered, where X is half of the total words over a threshold of interestingness (set as a score under 0.2). A minimal value of X is set (but, with sufficient amounts of data, almost never needed).

The scores for the target folder and for the opponent folder are found by multiplying all the interesting words' scores together and scaling the results such that they sum to one.

## Multiple classes

Each class is considered against every other class, and the resulting probabilities are multiplied together. This is based on the assumption that the correct class should beat every other class, so all classes are equally valid as controls. It also means that adding a new class that is most definitely not the correct one (all probabilities against it will be close to 1) doesn't affect the scores of other classes much. In practice, the scores between incorrect classes are hard to predict, but the correct class is clear enough.

The results are scaled up to sum to 1. This is mostly done for reasons of aesthetics, since rarely do more than two folders get non-negligible scores. If the necessary scaling is significant (i.e. every folder has good evidence going against it), the correct class may be missing completely.

## Hierarchical classing

If a class contains subclasses, their content is considered content of the parent class. Classification always works between the subfolders of a single folder, with their further internal structure being irrelevant.

The purpose of hierarchical classing is to class based on different criteria. For example, different criteria have to be used to classify articles (primarily topic-based) than commercial content (primarily purpose-based). 

## Logarithmic representation

To avoid trouble with floating point precision, all probabilities throughout classing are represented as their natural logarithms. They are also displayed as such in the multi-class explainer.

# Data sourcing

There is a potential for bias in the source data. Klaus only understands a class in terms of the files that are placed within it, and thus can pick up on irrelevant clues. This problem cannot be fixed completely: If there existed an unbiased, comprehensive directory of the Internet by topic to source the data from, there would be no need for this program to exist.

To mitigate it, several diversifying measures are used.

1) Source data from multiple domains.
2) For each domain, find text that always occurs on it, which is typically boilerplate (such as a header with its name). Remove such text as an invalid clue.
3) Try to divide any folder into different ways of approaching it. For instance, "food" can contain recipes or food safety. The lowest levels of the hierarchy are less important as targets (indeed, with under 5 files, classing is myopic at best) and more as specifications of the variations on the topic that should be included.
4) Try to include these subclasses in appropriate amounts. If one subfolder is unduly dominating (typically because there exists plenty of example content), set a weight lower than one for it. Its corpus will be scaled down when building the parent class corpus.
5) Run crawlers (focused and unfocused) and maintain a list of results (sites together with their determined locations). Only add misclassed results to their correct place, and do so proportionately to the degree of misclassing (if it happens on a very low level, where there is not enough training data, do not attempt to fix the problem yet). This avoids amplifying any incidentals that helped with the correct classification while increasing class diversity and representativeness.
6) If there is a persistent unclassifiable problem, a new class may be needed, as happened with *news*.
7) Check for class integrity. This crude tool attempts to class all training files as if they were new ones, which can show that some files don't appear to belong in their respective classes. This may be either due to misclassed files or underrepresented subcategories. It may, however, come up with false OK results caused by incidentals.
8) Use existing web directories (such as DMOZ) when possible, but don't copy everything from them, as they are sometimes outdated and use a different classification scheme.

## Class structure logic

The very first level is designed to filter vastly different groups of sites (*real-world* versus *info*), together with groups that would be better avoided (*content-mills*, *legal*). Some additional formats that don't belong under any superfolders and are easy to recognize are included (*weather-forecasts*, *dictionaries*). Classes besides *info* and *real-world* are only shallowly further divided into their various forms.

The *real-world* class serves to group the utility part of the internet, and thus further classes by site purpose. The *info* class subclasses by topic. This tends to create many, many basic levels, so all efforts are to ensure classes are truly disjunct.

### Data choice policy

It is intended to eventually have a class for any page the crawler may find on its journey. This is necessary because the crawler cannot determine that a given site does not have an appropriate class. In its absence, it often classes metaphorically (for example, esoteric/astrological sites as *religion* or *space*). 

# Crawling

Crawling functionality is provided. It respects robots.txt and properly identifies itself in request headers. It is only accessible by the REPL, not from the GUI. Pending class expansion, only an unfocused crawler is included.

## Bot behaviour

The system utilised (BackBot) always follows a random valid link (see below). If impossible, it places the current URL on a blacklist, then moves back. If it fails to find intra-domain links, it attempts to at least find an inter-domain link (note to self: this is dubious). It is optimised for speed - only considers links until it finds a valid one.

## Link choice limitations

To avoid getting stuck on a particular domain, the bot always stays on a single domain for a given number of steps (ignoring inter-domain links), then forcefully changes domains (ignoring intra-domain links and all domains it had already visited). All links with too few words, too many unknown characters, or too many unknown words are ignored.

## Zoombot valuation

This valuation will most likely be used for the focused crawler: Below a certain value (somewhere around 10<sup>-3</sup>), the actual probability is more influenced by the volume of text and coincidences than proximity of topics. Thus, in such cases, the bot should head in the most promising direction. A new scoring system works toward this goal: The score has two parts. First priority is the deepest folder that shows up as the first in classification (first all steps of the way). Second priority is the probability of the next level being correct. This would let the bot get somewhat close when there is no fully correct link, then eventually zoom in to the correct final folder. 

## Running the bot

BackBot starts with a seed and behaviour parameters. It places everything it finds in mock folders and downloads all found pages into the *files* folder. Currently, creating a separate *files* folder for it must be done manually. The old folder is not needed as long as the corpus had been rebuilt. Instead, one from an old crawl may be used to ban already known domains.

# UI

## Missing pieces

classes cannot be merged or removed programatically - this must be done by manually moving the underlying folders in "DATA/classes/", then rebuilding the corpus.

## REPL

The place of a given URL may be determined by (place url). Bots are called from the REPL; refer to the source code for parameter lists.

## GUI

### Classification explainer

(run) launches a more complex classifier display. "Blind" is to be used if you want explanation of how a given link would class if it weren't in the database already. "Explain classing" will launch an explainer window with scores of class pairs. Buttons within this window may be clicked to open details on a given pair, i.e. the chosen words, their occurrence numbers in rescaled corpora and the resulting probabilities.

### Database editing

Running (db) launches a database editor. Within it, one can make changes to the classes and their contents. Any added URLs get automatically downloaded. Changes will only be reflected in the behaviour of classing once the corpus is rebuilt.

#### Bucket

To move many files at once, one would typically use click, mark, and drag. Seeing as LTK lacks such a feature for buttons, the closest alternative is used, as inspired by http://wiki.c2.com/?HandVsPointer. A section labeled "bucket" is used where the user may place any links existing in folders. The bucket is preserved when moving through the database. After navigating to a target folder, links in the bucket may be acted on - copied, moved, removed from the original folder, or removed from the bucket. Only then do files actually get affected; the sole placing of a file into the bucket does nothing. This system has the additional advantage of making it easy to move many files between many folders, even ones distant in the folder structure, at the same time, which often happens when tweaking the classes.

Eventually, the bucket will only be used for moving or copying (which may end up deprecated - so far, no duplication is present in training files). The other options should happen locally.

# Design choices

## Natural language

I study articles in English because they are the most available. This creates numerous problems with multiple meanings of a given word, but avoids most problems related to different forms of a given word (stemming in English is manageable automatically for most words). A non-English folder is only added as a surface-level class, not further divided once the correct language is reached. It is only there to help if the crawler were to accidentally enter a non-English site.

## Programming paradigm

Common Lisp is a reasonably fast (when optimised) multi-paradigm language. The analytical part of the program (classification) is written in a mostly functional style, while text manipulation algorithms, data storage and retrieval, and crawlers are wrtitten imperatively. There is no need for OOP as all data strutures are simple - corpuses are hash tables, sites are strings - and without associated actions.

CL's main drawback is its shortage of well-made libraries, which is the most visible with the GUI.

## Data storage

# Speed

fast-substr-check is rought twice as fast as a full substring check in the average case

# Used libraries

Drakma (https://github.com/edicl/drakma) for downloading page data.

LTK (http://www.peter-herth.de/ltk/index.html) for the window GUI.

QURI (https://github.com/fukamachi/quri) for dealing with URL character issues (example.com/B%C3%A1g%C5%99i -> example.com/Bágři)

Plump (https://github.com/Shinmera/plump) for something similar inside the HTML (XML escape: \&rsquo; -> ' (with a little help))

cl-strings (https://github.com/diogoalexandrefranco/cl-strings) for string manipulation utilities.

# Other used software

Portacle
Github

# Code sources

The *letrec* macro was originally written by the user "tfb" on SO (https://stackoverflow.com/questions/63999450/lisp-variable-using-itself-in-definition). I rewrote it to avoid using loop.

The algorithm for overlap detection was adapted from https://neil.fraser.name/news/2010/11/04/.

# Other sources

Plenty of content I have little personal experience with was taken from the DMOZ directory (with additional filtering to suit the categories used here). Some aspects of the program are the result of consultations with the project advisor (Šimon Schierreich) and @simonbrecher.
